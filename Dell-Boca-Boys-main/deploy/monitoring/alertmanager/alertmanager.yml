global:
  resolve_timeout: 5m
  slack_api_url: '{{ SLACK_WEBHOOK_URL }}'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates for alert formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Routing tree
route:
  # Default receiver for all alerts
  receiver: 'team-notifications'

  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'component']

  # Wait time before sending first notification for a group
  group_wait: 30s

  # Wait time before sending updated notification for a group
  group_interval: 5m

  # Wait time before re-sending notification
  repeat_interval: 4h

  # Child routes for specific alert types
  routes:
    # Critical alerts - page on-call engineer
    - match:
        severity: critical
      receiver: 'pagerduty-critical'
      continue: true
      group_wait: 10s
      repeat_interval: 30m

    # Database alerts
    - match:
        component: database
      receiver: 'database-team'
      continue: false

    # Security alerts
    - match:
        component: security
      receiver: 'security-team'
      continue: true

    # Memory/AI system alerts
    - match_re:
        component: 'memory|agents'
      receiver: 'ai-team'
      continue: false

    # Workflow alerts
    - match:
        component: workflows
      receiver: 'workflow-team'
      continue: false

# Alert inhibition rules
inhibit_rules:
  # Inhibit warning-level alerts if critical alert is firing for same component
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['component', 'instance']

  # Inhibit database connection alerts if database is down
  - source_match:
      alertname: 'DatabaseDown'
    target_match_re:
      alertname: '.*Database.*'
    equal: ['instance']

  # Inhibit all alerts if orchestrator is down
  - source_match:
      alertname: 'OrchestratorDown'
    target_match_re:
      alertname: '.*'
    equal: ['cluster']

# Receivers configuration
receivers:
  # Default team notifications (Slack)
  - name: 'team-notifications'
    slack_configs:
      - channel: '#dell-boca-boys-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          *Alert:* {{ .GroupLabels.alertname }}
          *Severity:* {{ .CommonLabels.severity }}
          *Component:* {{ .CommonLabels.component }}
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          *Details:* <{{ .ExternalURL }}|View in Prometheus>
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'

  # PagerDuty for critical alerts
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '{{ PAGERDUTY_SERVICE_KEY }}'
        description: '{{ .GroupLabels.alertname }}: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        details:
          severity: '{{ .CommonLabels.severity }}'
          component: '{{ .CommonLabels.component }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        client: 'Dell-Boca-Boys Orchestrator'
        client_url: '{{ .ExternalURL }}'

  # Database team
  - name: 'database-team'
    slack_configs:
      - channel: '#database-alerts'
        title: 'Database Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}

          Check PostgreSQL dashboard: <https://grafana.example.com/d/postgres|PostgreSQL>
        send_resolved: true
    email_configs:
      - to: 'database-team@example.com'
        from: 'alertmanager@dell-boca-boys.example.com'
        subject: '[{{ .Status }}] Database Alert: {{ .GroupLabels.alertname }}'

  # Security team
  - name: 'security-team'
    slack_configs:
      - channel: '#security-alerts'
        title: 'üîí Security Alert: {{ .GroupLabels.alertname }}'
        text: |
          *SECURITY ALERT*
          *Severity:* {{ .CommonLabels.severity }}
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}

          Immediate investigation required!
        send_resolved: true
        color: 'danger'
    email_configs:
      - to: 'security-team@example.com'
        from: 'alertmanager@dell-boca-boys.example.com'
        subject: '[SECURITY] {{ .GroupLabels.alertname }}'
        headers:
          X-Priority: '1'

  # AI/Memory team
  - name: 'ai-team'
    slack_configs:
      - channel: '#ai-system-alerts'
        title: 'ü§ñ AI System Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Component:* {{ .CommonLabels.component }}
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}

          Check AI dashboards: <https://grafana.example.com/d/ai-overview|AI Overview>
        send_resolved: true

  # Workflow team
  - name: 'workflow-team'
    slack_configs:
      - channel: '#workflow-alerts'
        title: '‚öôÔ∏è Workflow Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}

          Check workflow dashboard: <https://grafana.example.com/d/workflows|Workflows>
        send_resolved: true

# Time-based silencing (maintenance windows)
time_intervals:
  - name: weekday-business-hours
    time_intervals:
      - weekdays: ['monday:friday']
        times:
          - start_time: '09:00'
            end_time: '17:00'
        location: 'America/New_York'

  - name: weekend
    time_intervals:
      - weekdays: ['saturday', 'sunday']
