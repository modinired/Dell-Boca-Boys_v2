# Vito - Local Coding Agent Configuration

# ====================
# Qwen LLM Settings
# ====================

# Qwen endpoint (OpenAI-compatible API)
# Default: http://localhost:8000/v1 (for vLLM)
# Ollama: http://localhost:11434/v1
# llama.cpp: http://localhost:8080/v1
# LM Studio: http://localhost:1234/v1
QWEN_ENDPOINT=http://localhost:8000/v1

# Qwen model name
# For vLLM: Qwen/Qwen2.5-Coder-32B-Instruct
# For Ollama: qwen2.5-coder:32b
# For llama.cpp/LM Studio: path to your model
QWEN_MODEL=Qwen/Qwen2.5-Coder-32B-Instruct

# LLM parameters
TEMPERATURE=0.1            # Lower = more deterministic (0.0-1.0)
MAX_TOKENS=4096            # Maximum tokens in response
LLM_TIMEOUT=120            # Request timeout in seconds

# ====================
# Memory Settings
# ====================

# Enable persistent memory
ENABLE_MEMORY=true

# Memory database location
# Default: ~/.vito/memory.db
# MEMORY_DB_PATH=/custom/path/memory.db

# Maximum context length to include in prompts
MAX_CONTEXT_LENGTH=8000

# ====================
# Agent Settings
# ====================

# Agent name (for display)
AGENT_NAME=Vito

# Enable streaming responses
STREAMING=true

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ====================
# API Settings
# ====================

# API server host
API_HOST=0.0.0.0

# API server port
API_PORT=8080

# ====================
# Optional: Vito Home Directory
# ====================

# Directory for Vito data (memory, logs, etc.)
# Default: ~/.vito
# VITO_HOME=/custom/vito/directory
